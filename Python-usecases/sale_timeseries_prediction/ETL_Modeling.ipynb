{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Do ETL                                                               #\n",
    "########################################################################\n",
    "\n",
    "# Load data from csv file\n",
    "df = pd.read_csv(\"data/training.csv\", header=0)\n",
    "\n",
    "# Configurations\n",
    "NUM_PRODUCTS = 2\n",
    "NUM_PAST_DAYS = 15\n",
    "NUM_FUTURE_DAYS = 15\n",
    "\n",
    "# Form initial dataframe with separate columns for product series\n",
    "dic = {\"date\": df.TSDate.unique().tolist()}\n",
    "for i in range(1, NUM_PRODUCTS + 1):\n",
    "    dic[\"series{}\".format(i)] = df[df.serieNames==\"serie_{}\".format(i)] \\\n",
    "            .sales.reset_index().drop([\"index\"], axis=1).sales\n",
    "etl_df = pd.DataFrame(dic)\n",
    "\n",
    "# Scale numeric columns\n",
    "numeric_cols = [\"series{}\".format(i+1) for i in range(NUM_PRODUCTS)]\n",
    "scaler = MinMaxScaler().fit(etl_df.loc[:, numeric_cols])\n",
    "etl_df.loc[:, numeric_cols] = scaler.transform(etl_df.loc[:, numeric_cols])\n",
    "\n",
    "# Add more columns of days of week and days of month\n",
    "tf_date = pd.to_datetime(etl_df.date)\n",
    "etl_df[\"dayofweek\"] = tf_date.dt.dayofweek\n",
    "#etl_df[\"dayofmonth\"] = tf_date.dt.day\n",
    "\n",
    "# Factorize categorical features\n",
    "cat_cols = [\"dayofweek\"]#, \"dayofmonth\"]\n",
    "for col in cat_cols:\n",
    "    etl_df[col] = pd.factorize(etl_df[col])[0]\n",
    "\n",
    "# Fit onehot for categorical features\n",
    "onehot_encoder = OneHotEncoder()\n",
    "dummies = onehot_encoder.fit_transform(etl_df[cat_cols])\n",
    "etl_df = pd.concat([etl_df.drop(cat_cols, axis=1),\n",
    "                   pd.DataFrame(dummies.toarray())], axis=1)\n",
    "\n",
    "# Shift past and future days for all products\n",
    "for i in range(1, NUM_PRODUCTS + 1):\n",
    "    # Shift past days\n",
    "    for p_day in range(1, NUM_PAST_DAYS + 1):\n",
    "        etl_df[\"series{0}_p{1}\".format(i, p_day)] = etl_df.series1.shift(p_day)\n",
    "\n",
    "    # Shift future days\n",
    "    for f_day in range(1, NUM_FUTURE_DAYS + 1):\n",
    "        etl_df[\"series{0}_f{1}\".format(i, f_day)] = etl_df.series1.shift(-f_day)\n",
    "\n",
    "# Drop the date column\n",
    "etl_df.drop([\"date\"], axis=1, inplace=True)\n",
    "\n",
    "# Keep the last day for future prediction\n",
    "last_day = etl_df.iloc[-1]\n",
    "\n",
    "# Drop NaN values\n",
    "etl_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Develope machine learning model with cross-validation on time series #\n",
    "########################################################################\n",
    "\n",
    "def train_model(x_train, y_train, alpha=1e-3, hid_layers=[512], max_iter=100):\n",
    "    \"\"\"\n",
    "    Train model on training data.\n",
    "    :param x_train: training examples\n",
    "    :param y_train: target variables\n",
    "    :param alpha: L2 regularization coefficient\n",
    "    :param hid_layers: hidden layer sizes\n",
    "    :param max_iter: maximum number of iterations in L-BFGS optimization\n",
    "    :return a model trained with neuron network\n",
    "    \"\"\"\n",
    "    nn_model = MLPRegressor(solver='lbgfs', hidden_layer_sizes=hid_layers, \n",
    "                            alpha=alpha, max_iter=max_iter, \n",
    "                            activation=\"relu\", random_state=1)\n",
    "    nn_model.fit(x_train, y_train)\n",
    "    \n",
    "    return nn_model\n",
    "\n",
    "\n",
    "def timeseries_cv(x, y, kfolds=10, alpha=1e-3, hid_layers=[512], max_iter=100):\n",
    "    \"\"\"\n",
    "    Implement a cross-validation procedure for time series data.\n",
    "    :param x: training and validation examples\n",
    "    :param y: target variables\n",
    "    :param kfolds: number of folds for cross-validation\n",
    "    :param alpha: L2 regularization coefficient\n",
    "    :param hid_layers: hidden layer sizes\n",
    "    :param max_iter: maximum number of iterations in L-BFGS optimization\n",
    "    :return cross-validation training and validation loss\n",
    "    \"\"\"\n",
    "    # Number of examples in each fold\n",
    "    k = int(np.floor(float(x.shape[0]) / kfolds))\n",
    "    \n",
    "    num_loops, acc_train_loss, acc_val_loss = 0, 0, 0\n",
    "\n",
    "    # Loop from the first 2 folds\n",
    "    for i in range(2, kfolds + 1):\n",
    "        # Get current training and validation data, slide through time\n",
    "        x_ = x[:(k*i)]\n",
    "        y_ = y[:(k*i)]\n",
    "\n",
    "        # Define a split point for training and validation\n",
    "        split = float(i-1)/i\n",
    "        index = int(np.floor(x_.shape[0] * split))\n",
    "        \n",
    "        # Training folds\n",
    "        x_train = x_[:index]        \n",
    "        y_train = y_[:index]\n",
    "        \n",
    "        # Validation folds\n",
    "        x_val = x_[index:]\n",
    "        y_val = y_[index:]\n",
    "        \n",
    "        # Train model with current sliding data\n",
    "        model = train_model(x_train, y_train, alpha=alpha, \n",
    "                            hid_layers=hid_layers, max_iter=max_iter)\n",
    "\n",
    "        # Compute train and validation loss with current sliding data\n",
    "        train_loss = np.mean(np.mean((model.predict(x_train) - y_train)**2))\n",
    "        val_loss = np.mean(np.mean((model.predict(x_val) - y_val)**2))\n",
    "        \n",
    "        # Accumulate train and validation loss\n",
    "        acc_train_loss += train_loss\n",
    "        acc_val_loss += val_loss\n",
    "        num_loops += 1\n",
    "\n",
    "    return acc_train_loss/num_loops, acc_val_loss/num_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Model tuning, currently the code enables to tune 2 hyper parameters, #\n",
    "# namely hid_layers and alpha. The tuning can be measured by using     # \n",
    "# cross-validation evaluation. More hyper parameters can also be       #\n",
    "# extended easily.                                                     #\n",
    "########################################################################\n",
    "\n",
    "# Hyper parameters for tuning\n",
    "hid_layers = (50,)\n",
    "alpha = 1\n",
    "\n",
    "# Get target columns from the etl_df dataframe\n",
    "y_cols = [\"series{0}_f{1}\".format(i+1, j+1) for j in range(\n",
    "        NUM_FUTURE_DAYS) for i in range(NUM_PRODUCTS)]\n",
    "\n",
    "# Get feature columns from the etl_df dataframe\n",
    "x_cols = [col for col in etl_df.columns.tolist() if col not in y_cols]\n",
    "\n",
    "# Form x and y sets\n",
    "x = etl_df[x_cols]\n",
    "y = etl_df[y_cols]\n",
    "\n",
    "# Do cross-validation and draw training versus validation loss\n",
    "max_iters = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "kfolds = 10\n",
    "\n",
    "train_loss_cv = np.zeros(len(max_iters))\n",
    "val_loss_cv = np.zeros(len(max_iters)) \n",
    "for i, max_iter in enumerate(max_iters):\n",
    "    train_loss_cv[i], val_loss_cv[i] = timeseries_cv(x, y, kfolds=kfolds, \n",
    "                                                     alpha=alpha, \n",
    "                                                     hid_layers=hid_layers,\n",
    "                                                     max_iter=max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot train versus validation loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.xlim([0, 100])\n",
    "plt.ylim([0, 0.02])\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(max_iters, train_loss_cv, label=\"Training cross validation loss\")\n",
    "plt.plot(max_iters, val_loss_cv, label=\"Validation cross validation loss\")\n",
    "plt.legend(shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Answer questions                                                     #\n",
    "########################################################################\n",
    "\n",
    "# First, train model with best hyper-parameters found after tuning\n",
    "model = train_model(x, y, alpha=alpha, hid_layers=hid_layers, \n",
    "                    max_iter=100)\n",
    "\n",
    "\n",
    "# Predict for the next 15 days from 2015/11/16 to 2015/11/30\n",
    "y_pred = model.predict(last_day[x_cols])\n",
    "\n",
    "# Unscale the predicted values to the original ranges\n",
    "for i in range(NUM_FUTURE_DAYS):\n",
    "    y_pred[:, (2*i):(2*(i+1))] = np.round(scaler.inverse_transform(y_pred[:, (2*i):(2*(i+1))]))\n",
    "\n",
    "print \"Predict of Product 1: {}\".format(y_pred[0, 0::2])\n",
    "print \"Predict of Product 2: {}\".format(y_pred[0, 1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graphical output with the model fitting (2013/06/21 to 2015/11/15)\n",
    "y_fit = model.predict(x)\n",
    "y_tru = np.array(y)\n",
    "\n",
    "# Unscale the predicted values to the original ranges\n",
    "for i in range(15):\n",
    "    y_fit[:, (2*i):(2*(i+1))] = scaler.inverse_transform(y_fit[:, (2*i):(2*(i+1))])\n",
    "    y_tru[:, (2*i):(2*(i+1))] = scaler.inverse_transform(y_tru[:, (2*i):(2*(i+1))])\n",
    "\n",
    "# Draw fitting\n",
    "for j in range(NUM_FUTURE_DAYS):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 3))\n",
    "    axes[0].set_title(\"Series1, future day {}\".format(j+1))\n",
    "    axes[0].set_xlabel(\"Day\")\n",
    "    axes[0].set_ylabel(\"Sale\")\n",
    "    axes[0].plot(range(y_tru.shape[0])[-120:], y_tru[:, 2*j][-120:], label=\"Data\")\n",
    "    axes[0].plot(range(y_fit.shape[0])[-120:], y_fit[:, 2*j][-120:], label=\"Fitted\")\n",
    "    axes[0].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, shadow=True)\n",
    "   \n",
    "    axes[1].set_title(\"Series2, future day {}\".format(j+1))\n",
    "    axes[1].set_xlabel(\"Day\")\n",
    "    axes[1].set_ylabel(\"Sale\")\n",
    "    axes[1].plot(range(y_tru.shape[0])[-120:], y_tru[:, 2*j + 1][-120:], label=\"Data\")\n",
    "    axes[1].plot(range(y_fit.shape[0])[-120:], y_fit[:, 2*j + 1][-120:], label=\"Fitted\")\n",
    "    axes[1].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, shadow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement grid search for hyper parameter tuning based on \n",
    "# cross-validation. Currently it only supports for 2 hyper \n",
    "# parameters, namely hid_layers and alpha.\n",
    "def grid_search(x, y):\n",
    "    # Define a grid of hyper parameters\n",
    "    hid_layers_list = [(40,), (45,), (50,), (20, 20,), (25, 20,), (25, 25)]\n",
    "    alpha_list = [0.001, 0.01, 0.1, 1, 1.1, 1.2, 1.3]\n",
    "    \n",
    "    best_hid_layers, best_alpha, best_loss = None, None, 1e9\n",
    "    for hid_layers in hid_layers_list:\n",
    "        for alpha in alpha_list:\n",
    "            _, val_loss_cv = timeseries_cv(x, y, kfolds=10, \n",
    "                                           alpha=alpha, \n",
    "                                           hid_layers=hid_layers,\n",
    "                                           max_iter=100)\n",
    "            # Update best hyper parameters\n",
    "            if val_loss_cv < best_loss:\n",
    "                best_hid_layers = hid_layers\n",
    "                best_alpha = alpha\n",
    "                best_loss = val_loss_cv\n",
    "    \n",
    "    return best_hid_layers, best_alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
